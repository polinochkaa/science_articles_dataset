ИСПОЛЬЗОВАНИЕ ТЕХНОЛОГИЙ ПАРАЛЛЕЛЬНЫХ ВЫЧИСЛЕНИЙ
ПРИ МОДЕЛИРОВАНИИ МЕТАЛЛИЧЕСКИХ ФОТОННЫХ КРИСТАЛЛОВ
М. В. Давидович1
, П. А. Шиловский2
, Д. К. Андрейченко3
1Доктор физико-математических наук, профессор кафедры радиотехники и электродинамики, Саратовский государственный университет им. Н. Г. Чернышевского, DavidovichMV@info.sgu.ru
2
Аспирант кафедры математического обеспечения вычислительных комплексов и информационных систем, Саратовский
государственный университет им. Н. Г. Чернышевского, ShilovskyPA@info.sgu.ru
3Доктор физико-математических наук, доцент кафедры математического обеспечения вычислительных комплексов и
информационных систем, Саратовский государственный университет им. Н. Г. Чернышевского, AndreichenkoDK@info.sgu.ru
В работе рассматриваются возможности использования технологий параллельных вычислений Message Passing Interface
и Open Computing Language при моделировании металлических фотонных кристаллов методом функций Грина и интегральных уравнений. Анализируется эффективность этих технологий в рамках данной задачи, приводятся выводы о
целесообразности их применения.
Ключевые слова: параллельные вычисления, MPI, OpenCL, фотонные кристаллы.
ВВЕДЕНИЕ
Металлические фотонные кристаллы (МФК) в приближении идеальной проводимости металлических объектов широко исследовались последние три десятилетия [1—7]. Для анализа МФК используются аналитические [1—3, 5], прямые численные [4] и численно-аналитические методы [5—7],
основанные на методе функций Грина и интегральных уравнений. Последние позволяют получить
дисперсионные уравнения (ДУ) в замкнутой форме в приближении тонких проволочных включений
и с учетом одной или нескольких гармоник тока. Решение таких дисперсионных уравнений требует
большое количество вычислений, что делает практически невозможным получение результатов моделирования за адекватное время традиционными последовательными способами. Поэтому актуальным
становится модификация существующих или разработка новых методов для запуска на вычислительных системах с использованием различных технологий параллельных вычислений. В рамках данной
статьи будут рассмотрены различные аспекты использования технологий Message Passing Interface
(MPI) и Open Computing Language (OpenCL) при моделировании МФК. Цель работы — проанализировать эффективность использования технологий параллельных вычислений MPI и OpenCL при
моделировании металлических фотонных кристаллов.
1. ОБЗОР ТЕХНОЛОГИИ MPI
Message Passing Interface — это спецификация для написания библиотек передачи сообщений,
разработанная как стандарт для систем с распределённой памятью, систем передачи сообщений и параллельных вычислений. MPI основан на модели передачи сообщений, в которой данные из адресного
пространства одного процесса передаются в адресное пространство другого процесса посредством передачи сообщений.
Программная модель MPI представлена следующими элементами.
• Группы процессов. Каждая группа определяет упорядоченную коллекцию процессов и область
видимости при операциях взаимодействия.
• Коммуникаторы. Объекты данного типа предоставляют все операции взаимодействия в MPI.
Каждый процесс для коммуникатора имеет свой независимый номер, и все процессы составляют
упорядоченную топологию. Коммуникаторы делятся на два типа:
– интра-коммуникаторы. Обеспечивают взаимодействие между процессами из одной группы.
– интер-коммуникаторы. Обеспечивают взаимодействие между двумя группами процессов.
• Точка-точка операции. Осуществляют взаимодействие между двумя процессами из одной группы. Часто используемым примером является вызов MPI_Send, который позволяет внутри одной
группы передать данные одного процесса другому.
°c Давидович М. В., Шиловский П. А., Андрейченко Д. К., 2013
М. В. Давидович и др. Использование технологий параллельных вычислений
• Коллективные операции. Осуществляют взаимодействие среди всех процессов в группе. Типичным примером является вызов MPI_Bcast, который берёт данные из одного процесса и
отправляет их всем остальным процессам из той же группы.
• Типизация операций. Многие операции в MPI требуют, чтобы был указан тип передаваемых
данных. Хотя стандартом MPI определены некоторые типы данных (MPI_INT, MPI_DOUBLE
и т. д.), возможно создавать свои собственные типы и использовать их для передачи в сообщениях [8, 9].
2. ОБЗОР ТЕХНОЛОГИИ OPENCL
Open Computing Language — это промышленный открытый стандарт программирования под различные платформы, в том числе центральные и графические процессоры. Он включает в себя язык
программирования, интерфейс программирования приложений, библиотеки и системное окружение
для разработки программного обеспечения.
Для описания стандарта используют иерархию следующих моделей.
• Модель платформы. Состоит из хоста (запускающего устройства), на котором расположено
одно или несколько OpenCL-совместимых устройств. Каждое из таких устройств разделено
на выполняющие модули, состоящие из элементов обработки, производящих вычисления.
• Модель выполнения. Выполнение разделено на две части:
– хост-программа. Задаёт контекст выполнения вместе с индексным пространством OpenCL
программы (ядра);
– ядро. Выполняется непосредственно на OpenCL устройстве. Каждому элементу индексного
пространства сопоставляется свой экземпляр ядра — рабочего элемента, выполняющегося
на элементе обработки устройства. Рабочие элементы объединяются в группы.
• Модель памяти. Каждый рабочий элемент имеет доступ к четырём областям памяти:
– глобальная память. Предоставляет доступ на чтение и запись для всех рабочих элементов
из всех групп;
– постоянная память. Остаётся постоянной в течение всего времени выполнения ядра;
– локальная память. Разделяется внутри одной группы рабочих элементов и доступна на
чтение и запись;
– личная память. Относится к одному рабочему элементу и не доступна для остальных.
• Программная модель. Поддерживаются следующие программные модели:
– модель параллельных данных. Определяет вычисление в виде последовательности инструкций, применённых к нескольким элементам объекта памяти. Индексное пространство, связанное с выполнением OpenCL программы, определяет рабочие элементы и соотношение
данных с ними;
– модель параллельных заданий. Определяет процесс вычисления, при котором каждый экземпляр ядра выполняется независимо от индексного пространства [10].
3. МАТЕМАТИЧЕСКАЯ ПОСТАНОВКА ЗАДАЧИ МОДЕЛИРОВАНИЯ МФК
Рассмотрим трехмерный МФК, состоящий из металлических проволочных стержней с длиной l и
радиусом проволочек r, периодически расположенных по осям x, y, z, с периодами соответственно a,
b и c в среде (матрице) с диэлектрической проницаемостью ε. Проволочки считаем тонкими: r << l,
l < min(a,b,c) и ориентированными по оси z. Согласно модели ток течет только по оси проволочки,
а его плотность имеет вид
J = z0δ(x)δ(y)
X
N
s=1
Is cos(ksz), (15)
где z0 — орт-вектор оси z, N — количество учитываемых гармоник тока, ks = (2s − 1)π/l.
Для МФК будем использовать скалярную ФГ периодически расположенных сфазированных источников (периодическую ФГ) [5]. Плотность тока (15) создаёт только одну z компоненту векторпотенциала Az, через которую можно выразить электрическое и магнитное поля. Для первого имеем:
E =
grad(div) + k
2
0
ε
iwε0ε
z0Az. (16)
Информатика 87
Изв. Сарат. ун-та. Нов. сер. Сер. Математика. Механика. Информатика. 2013. Т. 13, вып. 2, ч. 1
Далее нам потребуется только компонента Ez электрического поля, так как только для нее следует
учитывать граничные условия. После интегрирования (15) с ФГ получаем Az, что согласно (16) дает
Ez =
2
iwε0εabc
X
N
s=1
(−1)s
Isks
X∞
m,n,k=−∞
cos µ
kzkl
2
¶
(k
2
0
ε − k
2
zk)exp(−i[kxmx + kyny + kzkz])
(k
2
kz − k
2
s
)(k
2
xm + k
2
yn + k
2
zk − k
2
0
ε)
. (17)
Компонента (17) должна обратиться в нуль на поверхности проволочки. Применяя метод Галеркина, имеем:
l/2
Z
−l/2
Ez(x,y,z)cos(ks
′z)dz = 0. (18)
В (18) точка (x,y) принадлежит окружности: x
2 + y
2 = r
2
. Целесообразно усреднить по всем
точкам окружности, записав x = r cos(ϕ), y = r sin(ϕ) и проинтегрировав по углу. При этом возникают
функции Бесселя:
1
2π
Z
2π
0
exp(±ir(kxm cos(ϕ) + kyn sin(ϕ)))dϕ = J0(r
q
k
2
xm + k
2
yn),
вместо экспонент получим:
amn(kx,ky) = J0(r
q
k
2
xm + k
2
yn). (19)
Соотношения (17) и (18) приводят к системе линейных алгебраических уравнений, определитель
которой ∆ должен быть равен нулю. Собственно,
F(k0, k) = ∆ = 0
и есть искомое ДУ. Теперь матричные элементы согласно (18) и (19) можно записать так:
Ess′ =
4ksks
′
iwε0εabc (−1)s+s
′ X
M
m,n,k=−M
cos2
(
kzkl
2
)
(k
2
0
ε − k
2
zk)amn(kx,ky)
(k
2
kz − k
2
s
)(k
2
kz − k
2
s
′ )(k
2
xm + k
2
yn + k
2
zk − k
2
0
ε)
,
где (M + 1)3 — количество учитываемых плоских волн (пространственных гармоник).
Решая ДУ, получим зонные диаграммы, на основе которых можно выразить диэлектрическую
проницаемость. Данный метод может быть модифицирован для решения кольцевых и квадратных
проволочных структур, что позволяет рассчитывать различные конфигурации МФК.
4. АНАЛИЗ ЭФФЕКТИВНОСТИ ТЕХНОЛОГИЙ ПАРАЛЛЕЛЬНЫХ ВЫЧИСЛЕНИЙ
Процесс решения ДУ представляет собой разбиение области поиска корня на участки и поиска
такого участка, на котором функция F(k0, k) меняет знак. Заметим, что наличие полюсов при смене
знака в знаменателе (17) делает применение более эффективных методов поиска нецелесообразным.
В связи с этим возможно распараллеливание вычислений, при котором каждому процессу отдаётся
своя область поиска. В данном случае удобно использовать технологию MPI.
Для расчёта была взята структура со следующими входными параметрами: l = 0.5, r = 0.05,
a = b = c = 1, M = 10, N = 1, kz = 0, область поиска k0 — от 0.1 до 6.2 с шагом 0.005. Значения kx
и ky взяты из диапазона между точек M(π/a,π/b), G(0, 0), X(π/a, 0), M(π/a,π/b) по 10 значений из
каждого отрезка.
В роли параллельной вычислительной системы выступала связка из трёх машин с двумя четырёхъядерными процессорами Intel Xeon E5345 @ 2.33ГГц на каждой — всего 24 ядра. Была использована
широко известная реализация стандарта MPI – MPICH2 [11]. Вычисления проводились в числах с
плавающей точкой двойной точности, учитывалось среднее из трёх запусков.
Для конфигураций с различным числом работающих процессов в табл. 1 приведены величины
времени выполнения в секундах и ускорения в сравнении с последовательным вариантом. Для сохранения точности данные приведены с учётом сотых долей.
88 Научный отдел
М. В. Давидович и др. Использование технологий параллельных вычислений
Таблица 1
Зависимость времени выполнения и ускорения от числа работающих процессов
Число Время, с Ускорение
процессов
1 117,38 1
4 29,44 3,99
8 14,82 7,92
12 9,96 11,78
16 7,67 15,30
20 6,21 18,90
24 5,35 21,94
Как можно видеть из результатов, конфигурация с 24
работающими процессами справляется с аналогичной задачей в 21.94 раза быстрее, чем последовательный вариант, что лишь на 8,6% меньше, чем максимально возможные показатели. Конечно, рост величины ускорения будет замедляться с дальнейшим увеличением количества
процессов из-за возрастания нагрузки на сеть. В нашем
случае процессы лишь обмениваются итоговыми результатами для каждого kx и ky, что делает нагрузку на сеть
минимальной. Таким образом, использование технологии
MPI оправданно в задаче поиска корня при решении ДУ.
Далее заметим, что при решении ДУ требуется вычисление суммы тройного ряда для каждого матричного элемента. Здесь целесообразно применить
параллельные вычисления. Для расчёта суммы ряда была применена технология OpenCL.
В качестве вычислительной системы выступала связка из двухъядерного процессора Intel Pentium
E2140 @ 2.81 ГГц и графического ускорителя AMD Radeon HD 6950, поддерживающего технологию
OpenCL (E2140 + HD6950). Входные данные были взяты из предыдущего примера.
Для проверки эффективности данной технологии была получена зависимость времени выполнения от размерности рядов. Для сравнения были приведены измерения для 24-ядерной вычислительной системы, рассмотренной выше (6 × E5345). Полученные значения для наглядности округлены
до целых секунд. Результаты представлены в табл. 2.
Таблица 2
Зависимость времени выполнения от размерности рядов
Размерность 6 × E5345 E2140 + HD6950
ряда время, с время, с
9261 5 48
68921 44 52
226981 151 77
531441 355 120
1030301 693 198
По результатам видно, что при использовании графического ускорителя и технологии OpenCL время
вычислений растёт медленнее, чем размерность ряда. Особено это заметно в лёгких режимах, где основную роль на себя берут накладные расходы, связанные с переключением контекста между центральным процессором и графическим ускорителем. В случае же 24-ядерной системы можно наблюдать практически линейное увеличение времени вычислений при
увеличении размерности рядов.
Рассмотрим абсолютные показатели обеих конфигураций. В лёгких режимах, где велика роль
накладных расходов на выполнение OpenCL приложения, система на базе графического ускорителя
серьёзно проигрывает системе из 24 ядер. В тяжёлых режимах наблюдается обратная ситуация. В
самом ресурсоёмком случае, при количестве элементов ряда, равном 1030301, применение технологии
OpenCL и рядового графического ускорителя даже при маломощном центральном процессоре позволило добиться выигрыша в скорости более чем в 3 раза. Таким образом, можно сделать вывод, что
технологию OpenCL с данным графическим ускорителем выгодно использовать при расчёте рядов
достаточно большой размерности.
ЗАКЛЮЧЕНИЕ
В результате выполнения данной работы была доказана эффективность использования технологий
параллельных вычислений MPI и OpenCL в решении задачи моделирования металлических фотонных
кристаллов. Данные технологии позволяют существенно сократить время вычислений, что делает их
применение целесообразным.
